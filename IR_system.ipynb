{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e673a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import math\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e60d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbm(query):\n",
    "    #importing the objects as pickle files\n",
    "    file1=open('posting_list.pkl',\"rb\")\n",
    "    posting_lists=pickle.load(file1)\n",
    "\n",
    "    file2=open('file_idx.pkl','rb')\n",
    "    files=pickle.load(file2)\n",
    "\n",
    "    #set of stopwords\n",
    "    stpwrds=set(stopwords.words('english'))\n",
    "\n",
    "    regex = re.compile('[^a-zA-Z\\s]')\n",
    "    words = re.sub(regex,'',query)\n",
    "\n",
    "    #tokenizing and stemming the query\n",
    "    tokens = [word.lower() for word in word_tokenize(words)]\n",
    "    stmd_tkns = [ps.stem(token) for token in tokens if token not in stpwrds]\n",
    "\n",
    "    #finding which files contain all the query tokens \n",
    "    dict_files = {}\n",
    "    for i in range(len(files)):\n",
    "        dict_files[i] = 0\n",
    "\n",
    "    for w in stmd_tkns:\n",
    "        if w in posting_lists:\n",
    "            for i in posting_lists[w].keys():\n",
    "                dict_files[i] += 1\n",
    "\n",
    "    #Extracting the file names\n",
    "    results = []\n",
    "    for i in dict_files.keys():\n",
    "        if dict_files[i]==len(stmd_tkns):\n",
    "            results.append(files[i])\n",
    "\n",
    "    results = sorted(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1e8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query):\n",
    "\n",
    "    #loading pickle files\n",
    "    file1 = open('df.pkl','rb')\n",
    "    df_dict = pickle.load(file1)\n",
    "\n",
    "    file2 = open('file_idx.pkl','rb')\n",
    "    file_lst = pickle.load(file2)\n",
    "\n",
    "    file3 = open('doc_norm.pkl','rb')\n",
    "    l2_norm_dict = pickle.load(file3)\n",
    "\n",
    "    file4 = open('doc_words.pkl','rb')\n",
    "    tot_tkns = pickle.load(file4)\n",
    "\n",
    "    file5 = open('doc_len.pkl','rb')\n",
    "    doc_len = pickle.load(file5)\n",
    "\n",
    "    file6 = open('posting_list.pkl','rb')\n",
    "    tf_dict = pickle.load(file6)\n",
    "\n",
    "    #set of stopwords\n",
    "    stpwrds = set(stopwords.words('english'))\n",
    "\n",
    "    regex = re.compile('[^a-zA-Z\\s]')\n",
    "    words = re.sub(regex,'',query)\n",
    "\n",
    "    #tokenizing and stemming the query\n",
    "    tokens = [word.lower() for word in word_tokenize(words)]\n",
    "    stmd_tkns = [ps.stem(token) for token in tokens if token not in stpwrds]\n",
    "\n",
    "    tf_idf_qry = []\n",
    "    l2_norm = 0\n",
    "\n",
    "    for i in stmd_tkns:\n",
    "        tf_idf = 0\n",
    "        if i in tf_dict:\n",
    "            tf_idf = stmd_tkns.count(i) * math.log(len(file_lst)/df_dict[i])\n",
    "        tf_idf_qry.append(tf_idf)\n",
    "        l2_norm += tf_idf ** 2\n",
    "    l2_norm = math.sqrt(l2_norm)\n",
    "\n",
    "    tf_idf_qry = np.array(tf_idf_qry)/l2_norm\n",
    "\n",
    "    #calculating the score for each document for a given query\n",
    "    score={}\n",
    "    for i in range(len(file_lst)):\n",
    "        tf_idf_doc = []\n",
    "        for t in stmd_tkns:\n",
    "            c_tf = 0\n",
    "            if t in tf_dict and i in tf_dict[t]:\n",
    "                c_tf = (tf_dict[t][i]) * math.log(len(file_lst)/df_dict[t])\n",
    "            tf_idf_doc.append(c_tf)\n",
    "        tf_idf_doc = np.array(tf_idf_doc)/l2_norm_dict[i]\n",
    "        score[i] = np.dot(tf_idf_qry,tf_idf_doc)\n",
    "\n",
    "    #sort similarity score in descending order \n",
    "    score1=sorted(score.items(),key=lambda x:x[1],reverse=True) \n",
    "\n",
    "    result =[]\n",
    "    #printing the top 15 documents\n",
    "    count = 15\n",
    "    for i in score1:\n",
    "        if count == 0:\n",
    "            break\n",
    "        result.append(file_lst[i[0]])\n",
    "        count-=1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98099621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pickle files\n",
    "with open('posting_list.pkl','rb') as file1:\n",
    "    term_f=pickle.load(file1)\n",
    "\n",
    "with open('df.pkl','rb') as file2:\n",
    "    idf_dict=pickle.load(file2)\n",
    "\n",
    "with open('file_idx.pkl','rb') as file3:\n",
    "    file_lst=pickle.load(file3)\n",
    "\n",
    "with open('doc_len.pkl','rb') as file4:\n",
    "    doc_len=pickle.load(file4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd271952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the query\n",
    "def query_preprocess(query1):\n",
    "    #set of stopwords\n",
    "    N=len(file_lst)\n",
    "    stpwrds = set(stopwords.words('english'))\n",
    "    tempq = re.compile('[^a-zA-Z\\s]')  \n",
    "    query1 = re.sub(tempq,'',query1)   #except alphabets, remove all other letters\n",
    "    tokens=nltk.word_tokenize(query1)  #tokenization\n",
    "    tokens_lwr=[]\n",
    "    for i in tokens:\n",
    "        tokens_lwr.append(i.lower())\n",
    "    tkns=[]\n",
    "    \n",
    "    for p in tokens_lwr:               #token stemming\n",
    "        tkns.append(ps.stem(p))\n",
    "    tkns1=[]\n",
    "    \n",
    "    for p in tkns:                     #remove stopwords\n",
    "        if p not in stpwrds:\n",
    "            tkns1.append(p)\n",
    "    tkns_1=[]\n",
    "    \n",
    "    for p in tkns1:                    #remove tokens not present in any of the documents\n",
    "        if p in idf_dict:\n",
    "            tkns_1.append(p)\n",
    "    return tkns_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17967c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(p): #function for finding similarity score\n",
    "    doc_f=0\n",
    "    N=len(file_lst)\n",
    "    if p in idf_dict:\n",
    "        doc_f1=(len(file_lst)/idf_dict[p])\n",
    "    idf_val2=math.log((N-doc_f1+0.5)/(doc_f1+0.5))\n",
    "    return idf_val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea7a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(query):    \n",
    "    #set of stopwords\n",
    "    stpwrds = set(stopwords.words('english'))\n",
    "\n",
    "    count=0\n",
    "    N=len(file_lst)\n",
    "    for i in doc_len:\n",
    "        count=count+doc_len[i]\n",
    "    avg_len=count/N\n",
    "    avg_len\n",
    "\n",
    "    score={}\n",
    "    query_pp=query_preprocess(query)\n",
    "    k=1.2\n",
    "    b=0.75\n",
    "    for i in range(len(file_lst)):\n",
    "            score[i]=0\n",
    "            for j in query_pp:\n",
    "                tm_f=0\n",
    "                if j in term_f:\n",
    "                    if i in term_f[j]:\n",
    "                        tm_f=term_f[j][i]\n",
    "                idf_val=idf(j)\n",
    "                scr_i=idf_val*(k+1)*tm_f/(tm_f+k*(1-b+b*(doc_len[i]/avg_len)))  #similarity score for document and query\n",
    "                score[i]+=scr_i\n",
    "\n",
    "    #sorting similarity score in descending order\n",
    "    score1=sorted(score.items(),key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    result = []\n",
    "    #print the top 15 documents\n",
    "    count = 15\n",
    "    for i in score1:\n",
    "        if count == 0:\n",
    "            break\n",
    "        result.append(file_lst[i[0]])\n",
    "        count-=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b0b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('query.txt','r') as file:\n",
    "    q_file = file.read()\n",
    "    file.close()\n",
    "    \n",
    "q_dict = {}\n",
    "for l in q_file.split('\\n'):\n",
    "    if(len(l.split('\\t'))==2):\n",
    "        q_id,query = l.split('\\t')\n",
    "        q_dict[q_id] = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c45aa19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smb = 'QueryId,Iteration,DocId,Relevance'\n",
    "tfidf = 'QueryId,Iteration,DocId,Relevance'\n",
    "bmtf = 'QueryId,Iteration,DocId,Relevance'\n",
    "for q_id in q_dict:\n",
    "    qry = q_dict[q_id]\n",
    "    \n",
    "    q_list = sbm(qry)\n",
    "    c = 0\n",
    "    for i in q_list:\n",
    "        if c == 15:break\n",
    "        c += 1\n",
    "        smb = smb + str('\\n'+q_id+','+'1,'+i+','+'1')\n",
    "    \n",
    "    k = 0\n",
    "    while (c<15):\n",
    "        if file_lst[k] not in q_list:\n",
    "            smb = smb + str('\\n'+q_id+','+'1,'+file_lst[k]+','+'0')\n",
    "            c += 1\n",
    "        else:\n",
    "            k += 1\n",
    "    \n",
    "    q_list = tf_idf(qry)\n",
    "    c = 0\n",
    "    for i in q_list:\n",
    "        if c == 15:break\n",
    "        c += 1\n",
    "        tfidf = tfidf + str('\\n'+q_id+','+'1,'+i+','+'1')\n",
    "    \n",
    "    k = 0\n",
    "    while (c<15):\n",
    "        if file_lst[k] not in q_list:\n",
    "            tfidf = tfidf + str('\\n'+q_id+','+'1,'+file_lst[k]+','+'0')\n",
    "            c += 1\n",
    "        else:\n",
    "            k += 1 \n",
    "    \n",
    "    \n",
    "    q_list = bm25(qry)\n",
    "    c = 0\n",
    "    for i in q_list:\n",
    "        if c == 15:break\n",
    "        c += 1\n",
    "        bmtf = bmtf + str('\\n'+q_id+','+'1,'+i+','+'1')\n",
    "    \n",
    "    k = 0\n",
    "    while (c<15):\n",
    "        if file_lst[k] not in q_list:\n",
    "            bmtf = bmtf + str('\\n'+q_id+','+'1,'+file_lst[k]+','+'0')\n",
    "            c += 1\n",
    "        else:\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93eb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Boolean.csv','w') as file:\n",
    "    file.write(smb)\n",
    "    file.close()\n",
    "with open('Tf-Idf.csv','w') as file:\n",
    "    file.write(tfidf)\n",
    "    file.close()\n",
    "with open('BM25.csv','w') as file:\n",
    "    file.write(bmtf)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
